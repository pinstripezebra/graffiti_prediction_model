{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import os\n",
    "import shutil\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow_addons as tfa\n",
    "import tempfile\n",
    "import seaborn as sns\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model Structure\n",
    "\n",
    "Here we define our model structure using data augmentation to augment are training dataset and then call this function to create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height, image_width = 500, 500\n",
    "batch_size = 256\n",
    "epochs = 3\n",
    "\n",
    "def create_model(image_height, image_width, threshold):\n",
    "\n",
    "    # Defining data augmentation layer\n",
    "    data_augmentation = keras.Sequential([\n",
    "            keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "            keras.layers.RandomRotation(0.2),\n",
    "            ])\n",
    "    #Defining model\n",
    "    model = keras.Sequential([\n",
    "            data_augmentation,\n",
    "            keras.layers.Rescaling(1./255),\n",
    "            keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(image_height, image_width, 3)),\n",
    "            keras.layers.MaxPool2D((2, 2)),\n",
    "            keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "            keras.layers.MaxPool2D((2, 2)),\n",
    "            keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            keras.layers.MaxPool2D((2, 2)),\n",
    "            keras.layers.Flatten(),\n",
    "            keras.layers.Dense(512, activation='relu'),\n",
    "            keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "    #Compling model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                        optimizer='adam', \n",
    "                        metrics=[tfa.metrics.F1Score(num_classes = 2, threshold = threshold, average=\"micro\")])\n",
    "    return model\n",
    "\n",
    "def plot_result(history):\n",
    "    acc = history.history['F1Score']\n",
    "    val_acc = history.history['val_F1Score']\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cm(labels, predictions, threshold):\n",
    "  cm = confusion_matrix(labels, predictions > threshold)\n",
    "  plt.figure(figsize=(5,5))\n",
    "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "  plt.title('Confusion matrix @{:.2f}'.format(threshold))\n",
    "  plt.ylabel('Actual label')\n",
    "  plt.xlabel('Predicted label')\n",
    "\n",
    "  print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n",
    "  print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n",
    "  print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n",
    "  print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n",
    "  print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n",
    "\n",
    "def make_predictions(model, val_ds, threshold):\n",
    "\n",
    "    \"Takes model + validation dataframe as input, returns predictions and labels\"\n",
    "\n",
    "    labels =  np.array([])\n",
    "    predictions =  np.array([])\n",
    "    # Iterating through batches and making predictions\n",
    "    for x, y in val_ds:\n",
    "        labels = np.concatenate([labels, y.numpy()])\n",
    "        predictions = np.concatenate([predictions, \n",
    "                                        [round(i[0],0) for i in model.predict(x).tolist()]])\n",
    "\n",
    "    # Converting to 1/0 depending on value relative to threshold\n",
    "    predictions = [1 if i >= threshold else 0 for i in predictions]\n",
    "    # Converting to int type\n",
    "    predictions = np.array(predictions).astype(int)\n",
    "    labels = np.array(labels).astype(int)\n",
    "\n",
    "    return predictions, labels\n",
    "   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model for 500 x 500 image with a 0.5 classification threshold\n",
    "my_model = create_model(image_height, image_width, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "Reaing in the images from our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to read data from\n",
    "data_dir = './Data-Clean/'\n",
    "\n",
    "# Defining train an validation splits using 80/20 split\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  image_size=(image_height, image_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  image_size=(image_height, image_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model\n",
    "\n",
    "Now that we have our training data and model defined we can now train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "file_list = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "history = \"\"\n",
    "\n",
    "# only training if the model doesnt already exist then \n",
    "if \"my_model.pkl\" not in file_list:\n",
    "  history = my_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    "  )\n",
    "  joblib.dump(my_model, 'my_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Performence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First making predictions on test and train datasets\n",
    "train_predictions_baseline = my_model.predict(train_ds, batch_size=batch_size)\n",
    "test_predictions_baseline = my_model.predict(val_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels = make_predictions(my_model, val_ds, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions), len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating confusion matrix on test set\n",
    "plot_cm(labels, predictions, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining Model with Higher Classification Threshold\n",
    "This will help eliminate false positives and address our imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/63 [>.............................] - ETA: 18:07 - loss: 0.7661 - f1_score: 0.8568"
     ]
    }
   ],
   "source": [
    "low_threshold_history = \"\"\n",
    "high_threshold_history = \"\"\n",
    "low_threshold_model = create_model(image_height, image_width, 0.2)\n",
    "high_threshold_model = create_model(image_height, image_width, 0.8)\n",
    "\n",
    "# only training if the model doesnt already exist then \n",
    "if \"low_threshold_model.pkl\" not in file_list:\n",
    "  low_threshold_history = low_threshold_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    "  )\n",
    "  joblib.dump(low_threshold_model, 'low_threshold_model.pkl')\n",
    "if \"high_threshold_model.pkl\" not in file_list:\n",
    "  high_threshold_history = high_threshold_model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5\n",
    "  )\n",
    "  joblib.dump(high_threshold_model, 'high_threshold_model.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_high_threshold = low_threshold_model.predict(val_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_threshold_predictions, low_threshold_labels = make_predictions(low_threshold_model, val_ds, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll evluate performance of our second model with the 0.9 classification threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating confusion matrix on test set\n",
    "plot_cm(low_threshold_labels, low_threshold_predictions, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_threshold_predictions, high_threshold_labels = make_predictions(high_threshold_model, val_ds, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating confusion matrix on test set\n",
    "plot_cm(high_threshold_labels, high_threshold_predictions, 0.8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
